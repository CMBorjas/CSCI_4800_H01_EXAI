ABSTRACT
Several useful taxonomies have been published that survey the eXplainable AI (XAI) research field. However, these taxonomies typically do not show the relation between XAI solutions and several use case aspects, such as the explanation goal or the task context. In order to better connect the field of XAI research with concrete use cases and user needs, we designed the ASCENT (Ai System use Case Explanation oNTology) framework, which is a new ontology and corresponding metadata standard with three complementary modules for different aspects of an XAI solution: one for aspects of AI systems, another for use case aspects, and yet another for explanation properties. The descriptions of XAI solutions in this framework include whether the XAI solution has a positive, negative, inconclusive or unresearched relation with use case elements. Descriptions in ASCENT thus emphasize the (user) evaluation of XAI solutions in order to support finding validated practices for application in industry, as well as being helpful for identifying research gaps. Describing XAI solutions according to the proposed common metadata standard is an important step towards the FAIR (Findable, Accessible, Interoperable, Reusable) usage of XAI solutions.
1 INTRODUCTION
The field of eXplainable Artificial Intelligence (XAI) is rapidly progressing towards a large variety of technologies that generate human-interpretable explanations from AI systems. This momentum is boosted by the establishment of national and international regulations and legal frameworks that stress the importance of trustworthy and responsible AI. An example is the recent draft of the AI Act by the European Commission [10] which states that AI should be developed and applied in a responsible and trustworthy manner, generally following the moral imperative that AI should “do good” [ 9, 23]. Transparency and explainability are seen as two critical ingredients for this mission. XAI technologies are receiving increasingly more attention from the industry and academia, and many implementations are readily available (see Rothman [20] for an overview). Given the vast amount of XAI technologies, multiple survey papers [2, 3, 13 ] aim to provide an overview using different taxonomies. Most of these taxonomies, however, categorize XAI solutions mainly based on model characteristics and explanation types with little focus on the end-user. Moreover, they typically do not show the relation between XAI solutions and several use case aspects, such as the explanation goal or the task context. As a result, many of these taxonomies - viewed in isolation - mainly serve data scientists. In order to better connect the field of XAI research with use cases in industry, we argue that it is beneficial to also explicitly model the implications of an XAI solution for different use case aspects according to user evaluations, alongside categorizations of the AI model and the explanation that is desired. Because there are many different configurations of use cases, AI models, and explanation algorithms, we propose to model XAI solutions as instances of an ontology.
In this work, we propose the ASCENT (Ai System use Case Explanation oNTology) framework which consists of an ontology and corresponding metadata standard for annotating XAI solutions according to three ontology modules, namely AI System, Use Case, and Explanation Algorithm. This framework suits the growing focus within the XAI field towards more rigorous evaluation [8], incorporating scientific theories [12 , 15 , 26 ] and a human-centered approach [ 9 , 22]. The AI System module aligns with the technology- centered approach within XAI, as it describes the properties that signal what explanation can be generated (e.g. the type of data and model used). The Use Case module aligns with the growing human-centered approach to XAI and describes the important use case properties, which should be taken into account when searching for suitable XAI methods (e.g. the goal of an explanation). Finally, the Explanation Algorithm module describes the aspects of an explanation that need to be considered given these use case and AI system properties (e.g. the explanation generating method and the explanation’s modality).
The ASCENT framework allows a more FAIR [30 ] usage of XAI solutions by providing rich machine-readable metadata for XAI solutions (Findable), online access to our metadata standard (Accessible), the ontology in a commonly used OWL standard (Interoperable), and extension possibilities (Reusable). This paves the way for the creation of a communal repository of XAI solutions described according to a common metadata standard that is relevant for industry applications.
This paper is structured as follows. In the next section we address relevant literature on taxonomies and categorizations of the XAI field and emphasize our contribution. In Section 3, we present and motivate the proposed ontology and its modules. Section 4 elaborates on how the ASCENT framework is used in creating metadata of XAI methods. Finally, in Section 5, we will discuss and conclude on the proposed methodology
2 BACKGROUND
Some recent surveys of XAI methods use taxonomies to provide a clear overview of the XAI research field. Arrieta et al. [2] provide an in-depth overview of existing XAI methods, accompanied by two taxonomies. The first taxonomy provides a broad overview that allows for traversing a path through the taxonomy tree, based on the type of XAI solution or the model type. At the leaves, this taxonomy presents methods and their paper references, with additional color coding for the data type the method is applicable to. Belle and Papantonis [3] offer a similar but more succinct taxonomy, where the scope is explicitly limited to explanations of AI techniques that rely on statistical association. A second taxonomy by Arrieta et al . [2] specifically limits the scope to XAI methods for Deep Learning (DL) models such as Deep Neural Networks (DNNs), because these models tend to be the most problematic in terms of interpretability. Jin et al . [14] develop a prototyping tool for explanations that uses an extended version of the taxonomy presented in [ 13 ]. This work defines prototypical forms of explanations with UI/UX design templates and associated XAI algorithms.
XAI methods are in general not plug-and-play. For instance, they may require consideration of various user roles with different explanation needs. We observe that existing taxonomies mainly support the XAI research community and data scientists, but do not sufficiently address the societal need for tools that aid in the responsible application of XAI by factoring in user and use case properties. The literature in which such taxonomies are presented does however typically recognize that explanation is a difficult concept studied not only in AI, but also in philosophy and social sciences [e.g. 15 ]. For example, Arrieta et al . [2] propose a definition of explainable AI in which the target audience is factored in. Belle and Papantonis [3] show awareness of users and use cases by discussing a use case of a hypothetical data scientist that has to solve questions posed by business managers. The prototyping process designed by Jin et al. [14] is user-focused and was tested in user studies, but at the cost of explicitly limiting its scope to XAI methods “that do not require technical knowledge to comprehend” [14 ]. Nevertheless, the taxonomies themselves do not support a mapping to use case properties and leave this to the discretion of the data scientist. We instead propose an ontology that explicitly models use case properties alongside explanation and AI system properties.
Explanation ontologies have been published by [24] and [6], with the former being reused in the latter. [24 ] is a general purpose ontology, whereas [6] is specifically designed for user-centered AI explanations and thus has a similar aim as ASCENT. Our ontology however focuses specifically on XAI solutions and, as a result, we include insights from the several existing XAI taxonomies in greater detail. Having this more specific scope may improve the practical usability of the ontology, as well as provide a more fine-grained XAI-specific variant compared to the more general explanation ontologies. Another contribution of our ontology is that relations between a given XAI solution and use case aspects indicate whether this relation is researched and, if so, validated. This way, user evaluations are an integral part in the description of XAI solutions, rather than an afterthought.
3 ONTOLOGY
We define an overarching domain ontology for explainable AI applications with three modules (or sub-ontologies), namely Use Case (figure 1b), AI System (figure 1a) and Explanation Algorithm (figure 1c).
The main goal of the ontology is to describe the collection of entities that play a role when an XAI solution has to be identified based on a broad spectrum of requirements. The AI system is the main concept, to which both the use case (including the goals of the users) and the explanation algorithms are related. Figure 1d illustrates the relations between the three modules. A Use Case employs an AI System, which is employed in that Use Case. An AI System provides an explanation service by offering an Explanation Algorithm that explains the AI System. Various AI systems may be (potentially) involved in a use case. Additionally, various explanation algorithms may be applicable on a single AI system.
For each module we identify a first set of data elements that covers the majority of use cases, but we expect them to be expanded on for a specific use case. The main focus of our effort is not to replace existing taxonomies, but rather to incorporate their useful distinctions in a comprehensive framework that is useful for the development of practical applications. The AI system entity together with its associated use case and its associated explanation algorithms constitutes an environment that can be used in downstream applications. For example, it is possible to build a knowledge base with a predetermined set of annotated explanation algorithms that we can query.
Note that even though we have displayed all data elements like a taxonomy for convenience, they are not used as such. A taxonomy is used for classification, which in turn assumes you end up at a single leaf node. However, in our figures multiple leaf nodes can be applicable simultaneously. For example, it is no redundancy that the word “data” appears both under Explanandum Type and in Input Data. The question which input data type is required by an AI system is independent from the question whether it is the goal of a particular explanation algorithm to provide insight into a data set. Also note that most links in the figures indicate subclass relations in the underlying ontology, but that links with arrows indicate property relations. See section 4 for more details on the ontology serialization as OWL (Web Ontology Language).
3.1 Use Case
Three main use case characteristics are distinguished in the Use Case module, namely the background of the user, the goal of the explanation, and the task context (see figure 1b).
3.1.1 User Background. The background of the user affects which type of explanation is useful. We distinguish whether the user is an expert with respect to the application domain or the used AI, or the subject of the decisions supported by the AI, such as patients or consumers. For example, an explanation about the prediction of the most probable diagnosis cannot include complex medical terms when given to a patient, whereas a doctor probably needs a more in-depth and detailed explanation.
Within the expert group, an AI engineer typically focuses on debugging the AI model and how to improve its performance, while a domain expert wants to understand the underlying reasoning from the application perspective. Furthermore, consultants such as doctors and lawyers are typically interested in the reasoning behind the prediction of individual input samples, whereas researchers such as medical scientists and legislators are often also interested in more global explanations regarding multiple input samples.
3.1.2 Explanation Goal. An explanation serves a certain goal within a use case. We distinguish eight different explanation goals, which we illustrate with the use case of a doctor who is assisted by an AI model to diagnose and treat a patient:
Discover: AI can be used for the discovery of relevant patterns, such as clusters [7 ] and bias in the data [5 ]. For example, medical researchers may want to discover relevant relations between variables, such as diet and success of a treatment, by gaining insight from the AI model.
Decision Support: Explanations can aid decision making, for example by avoiding tunnel vision and providing actionable explanations. Doctors want to understand why a certain diagnosis is deemed most probable by the AI system, such that they can combine it with their own knowledge and expertise when deciding on the final diagnosis.
Calibrate trust: Explanations can aid in the appropriate use of an AI system by avoiding the extremes of over-reliance (blindly following the AI systems advice) and under-reliance (distrusting all output of the system) [27 ]. A doctor wants to understand when and why the AI system can be trusted.
Justify: After the doctor decided on the diagnosis with support of the AI system, an AI explanation can be used to justify the decision to the patient and in the medical report.
Audit: Auditors need to understand whether an AI system conforms to regulations. They for example may want to know which variables are used by the diagnosis model and how robust it is. 
Improve: Explanations can provide insights to improve the underlying AI model. For example, if it turns out that the model makes more mistakes for a particular patient group, the developer may decide that more representative data is needed.
Contest: When contesting a decision, users want to understand the reasoning behind that decision. This goal is becoming more relevant with the increased use of AI systems in daily life and regulations such as the GDPR containing the right to contest. For example, if a patient does not agree with the diagnosis, the decision may be contested by inquiring into which features were deemed relevant by the AI model. A possible outcome may be that the patient finds out that some irrelevant, wrong or inappropriate features were used.
Persuade: The type of explanation supporting a certain recommendation can affect how persuasive it is [25 ]. For example, the doctor may want to persuade the patient to stop smoking when the patient is developing lung disease. This goal is especially prevalent in retail use cases for recommendation of goods or services [25].
3.1.3 Task Context. The context of the task for which an AI model is used has an effect on which types of explanation are useful. Firstly, in use cases with high time pressure, such as collaboration between an AI and emergency medical responders, there is little time to understand in detail why a certain action is suggested by the AI system, while a GP, on the other hand, typically has more time to understand the reasoning behind an AI suggested diagnosis.
In high-risk domains such as health care, a detailed understanding of the underlying reasoning of an AI output is typically needed, as a mistake might have significant consequences. In low-risk applications such as recommendation systems in retail, the consumer typically has less need for a detailed understanding, for example why a piece of clothing is recommended.
3.2 AI System
We define three main characteristics of an AI System which affect the possible XAI solutions that can be used to extract explanations, namely the input data of the model, the model type and the task type it performs.
3.2.1 Input data. Six types of input data are considered, namely tabular, text, image, video, audio, and graph data. The characteristics of the input data of the model influence the possibilities of explanations. For example, data types that are visual in nature, such as images and graphs, allow for more visual explanations such as an attention layer on top of an image [31 ] or a subgraph of nodes which the model considered important [ 32 ]. Conversely, tabular data with large amount of features can be challenging to visualize and require a different type of explanation. 
3.2.2 Model Type. We distinguish supervised, unsupervised and reinforcement models, following the classification of Nicolas [18] . From an XAI perspective, supervised models have the advantage that their outputs are often human interpretable, even though the model itself can be very complex. Unsupervised models are more challenging and provide less interpretable output such assignments to opaque clusters, which in addition also should be presented in an interpretable way. At last, generating explanations for a Reinforcement Learning (RL) model can be challenging given the complex long-term behavior of RL agents. For a more detailed classification, we refer to existing taxonomies which classify different types of AI models [e.g. 4, 18].
3.2.3 Task. When specified on a fine granularity, many hundreds machine learning (sub)tasks may be formulated. We instead focus on common machine learning tasks that are typically addressed either in the XAI literature or in industry. The most common tasks are classification and regression in a supervised setting, and clustering and dimensionality reduction in an unsupervised setting. Our ontology is easily extendable with more specific tasks.
3.3 Explanation Algorithm 
We consider an explanation algorithm from the viewpoint of what is being explained (explanandum), the different explanation types, the various ways to present to the explainee (manner), and the strategy type used for explanation extraction.
3.3.1 Explanandum. Data: Because most AI applications nowadays are data-driven, it is important to explain properties of training and test data, e.g. for assessing potentially problematic bias in the AI system. Explanations may provide relevant data statistics as well as insights on how the data was collected. These insights may affect for which use cases a trained model is appropriate and can be recorded in standardized documentation of data sets [11].
Model Development: To ensure accountability for AI decisions it is also important to be able to explain and justify the design history of an AI system. We roughly distinguish four phases. In a design phase one may need to explain the choice for data set, whether a risk assessment was made, or why a particular model is appropriate given a concrete application domain. One may also ask how appropriate features were created and how possible issues are addressed during preprocessing. In the training phase, choices for hyper-parameter tuning or regularization are made. Although these are more technical in character, it is still important to be able to explain them e.g. explaining how undesirable consequences of overfitting on a particular subpopulation were mitigated. Finally, being able to explain the evaluation of the model is important for instilling trust in its responsible application. For example, why was the chosen evaluation metric appropriate given the goals and usage context of the AI? How is the error calibration of the model, e.g. was a model very certain of predictions that were in fact wrong?
AI functioning: A main focus of the XAI community has been to explain how an AI model functions, because contemporary sub- symbolic AI systems often have millions of interacting parameters contributing to the final output. We are often also interested in explaining the model’s confidence in its decisions for the sake of trust calibration. There is great variety in how models are explained, but in terms of what XAI methods explain, we distinguish local and global decision boundaries. Methods that explain decision boundaries locally aim to explain model behavior in a limited region of the model input space (e.g. the feature space close to a particular data instance), whereas explanations on global decision boundaries pertain to model behavior over the whole input space.
3.3.2 Explanation Type. There are various types of explanation for explaining the explanandum. Feature attribution methods quantify the influence of particular features on the outcome. This gives insight into which features are considered important for a prediction from the perspective of the model. One may also explain a prediction by showing examples of similar instances with similar outcomes, or the closest data instance with a different (counter factual) outcome. Prototypical examples may additionally provide insight in which types of instances are well represented in the data. One may also explain model behavior by providing rules that approximate and provide insight into the model’s decision boundary.
3.3.3 Manner. Explanations may be provided in all sensory modalities. Currently, explanations are provided either in the visual (which includes explanations via text) or auditory (e.g. in AI assistants) modality, but other modalities might also play a role.
Whereas many of the current XAI methods provide explanations statically, there is increasing attention for the fact that human explanations are socially interactive in nature [ 15 , 17 ]. For example, when a chat bot provides an explanation that is partially unclear, it is desirable that the explanation is adjusted or refined if the end user asks for clarification.
3.3.4 Strategy Type. If a model of choice has interpretable internals, one may take the transparent by design strategy. One can for example explain which features are important for a decision by investigating the coefficients in linear regression. In high-risk scenarios there is a strong argument for enforcing the transparent by design explanation strategy [ 21 ]. In other cases explainability has to be provided post-hoc. Some methods do this without considering model internals at all (model-agnostic), whereas other methods try to make model internals more interpretable (model-specific), for example by visualizing intermediate layers in convolutional neural networks.
4 TAGGING XAI SOLUTIONS USING ASCENT
This section describes how the proposed ontology can be used to
describe XAI solutions by “tagging” them with ontology elements
and illustrates this process with an example.
4.1 Method
We provide a metadata standard serialized as an OWL (Web Ontology Language) ontology1 based on the proposed model (section 3), such that the tagging procedure is formalized and can be shared. Tagging XAI solutions according to a common ontology is a deliberate effort that promotes the FAIR application of XAI solutions [30 ]. Providing a serialization of tagged XAI solutions makes these solutions easier to be unambiguously interpreted by the broader community. Modelling the standard as an OWL ontology also facilitates future extensions to and reuse of the model. OWL stems from the semantic web and linked data paradigms, so each concept of the ASCENT framework will be identifiable through a globally unique URI that external ontologies can relate to.
Tagging according to the Use Case module additionally promotes transparency with respect to appropriate usage contexts, thereby also addressing community calls for data sheets [11 ] and model
cards [16]. Another desirable side-effect is that manually tagging a given solution according to the Use Case module often requires an interdisciplinary effort, for example with social scientists, such that a broader societal perspective is included in the solution space from the outset.
To get rich descriptions, we require that XAI solutions are tagged with the most specific properties from the three ontology modules.2 Each ontology element has received a dedicated property “hasX” in the serialization, following OWL naming conventions. XAI solutions are of the type Explanation Algorithm and will require values for the “hasX” properties from the Explanation Algorithm module. The next step is to indicate relations with elements from the Use Case and AI System module. We currently define relations directly with leaf elements of the other modules for the sake of simplicity, but it is also possible to make separate instances for complex Use Case and AI System configurations and connect to those, if desired.
Indicating relations with AI System elements is relatively straight forward because they are typically static, but tagging with the Use Case module is often not. For example, even if counterfactual explanations are currently mainly connected with the explanation goal of “contesting” [29 , p. 40-41], it could be connected with “justify” or “improve” in future work. That is, the absence of a tag does not imply evidence that there is no connection. Yet, it is ambiguous what absence of a tag does imply, because it may mean that a relation is researched but shown not to be present or even negative, or that a relation has not been researched yet. Moreover, establishing relations with Use Case elements may require validation with user research. To enrich descriptions with information about user evaluations, we have implemented additional sub-properties in the ontology to disambiguate four possible types of relations: there is research supporting 1) a positive relation or 2) a negative relation3; 3) or a relation has been investigated with inconclusive results or 4) not researched at all (unknown). Research gaps and possibilities for further research can be identified by filtering on inconclusive or unresearched relations. XAI solutions should be exhaustively tagged using these relation properties if detailed filtering operations are desired in a downstream application.
4.2 Example
To illustrate the tagging procedure from the perspective of a group of experts adding an XAI solution to the knowledge base, we annotated LIME [19] with elements from the three modules (figure 2). 
The annotation with the AI system and Explanation Algorithm module is relatively straightforward, because it is clear what type of models and explanations LIME supports. The properties of the Explanation Algorithm are indicated with hasExplanandumType, hasExplanationType, hasManner, and hasStrategyType. The relations with elements from the AI system and Use Case module are indicated e.g. with positiveAssociationWith. Notice that in this simple case, we specify relations directly with data elements of the AI System and Use Case modules. In more complex scenarios, the ontology supports creating separate Use Case instances that an XAI solution can be associated with. This may be necessary if we want to associate an XAI solution with a particular combination of Use Case elements. The serialisation of such a Use Case instance would similarly use the properties hasExplanationGoal, hasTaskContext, involvesUserBackground, and so on. These properties are all implemented in the ontology and can be found in the referenced Turtle serialisation.
The annotation with the Use Case module relies more on insights from social science research. For example, one user study showed that even though users perceive feature attribution explanations as making the prediction of the model more transparent than no explanation, they actually perform poorer when asked to predict the model’s behavior after seeing the feature attribution explanation [1 ]. This suggests that feature attribution explanations have a negative relation with the goal of trust calibration. Research also indicates that model-agnostic strategies do not always generate faithful explanations [ 28 ]. This means LIME has a negative relation with high risk use cases in which the user needs to fully trust that the explanation reflects the true underlying reasoning of the model. Research gaps can be indicated via the unknownAssociationWith or inconclusiveAssociationWith element.
5 CONCLUSIONS
Existing XAI survey papers generally focus on categorization of XAI solutions according to different AI System and Explanation Algorithm properties. From the application perspective in the industry, however, it is important to view XAI solutions from the space of use case needs as well. This work integrates these three aspects in the ASCENT framework for annotating XAI solutions with three modules: AI System, Use Case, and Explanation Algorithm.
The aim of the ASCENT framework is to support the FAIR usage of XAI solutions by providing rich metadata to XAI solutions, online access to the underlying ontology, modelling the ontology in a commonly used standard, and by being extendable. This is an important step towards creating a communal repository of XAI solutions that are described with metadata relevant to industry applications. By requiring XAI solutions to be connected with use case elements, the framework encourages multidisciplinary collaboration, e.g. with social scientists.
As future work, we are planning to develop a shared tool with an interface for adding, managing, and filtering ASCENT descriptions, including evaluation results of XAI solutions. Given this knowledge base, the tool could provide validated recommendations of suitable XAI solutions, given specific use case needs. It is possible that for certain use case elements, no AI solution is available or properly evaluated yet. This provides a feedback loop to the XAI research community to focus more on those research gaps.
